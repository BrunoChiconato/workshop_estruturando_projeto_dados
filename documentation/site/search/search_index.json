{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bem-vindo","text":"<p>Este projeto implementa uma pipeline de dados automatizada para extrair, transformar e carregar (ETL) dados de vendas fict\u00edcias de diversos formatos de arquivo para um banco de dados PostgreSQL na nuvem. A pipeline foi projetada para consolidar dados de m\u00faltiplas fontes em um \u00fanico DataFrame, aplicar transforma\u00e7\u00f5es espec\u00edficas e armazen\u00e1-los em um reposit\u00f3rio central para facilitar a an\u00e1lise e a consulta.</p>"},{"location":"#objetivo-do-projeto","title":"Objetivo do Projeto","text":"<p>A pipeline ETL realiza as seguintes opera\u00e7\u00f5es principais:</p> <ol> <li> <p>Extra\u00e7\u00e3o (Extract): Extrai dados de arquivos <code>.csv</code>, <code>.json</code> e <code>.parquet</code> localizados em um diret\u00f3rio espec\u00edfico. Cada arquivo cont\u00e9m informa\u00e7\u00f5es de vendas, como identificadores de pedidos, clientes, produtos, m\u00e9todo de pagamento, quantidade e pre\u00e7o.</p> </li> <li> <p>Transforma\u00e7\u00e3o (Transform): Processa os dados consolidados para calcular o valor total das vendas, agrupando-os por m\u00e9todo de pagamento. Esse processo resulta em um resumo que permite uma an\u00e1lise mais f\u00e1cil e r\u00e1pida dos m\u00e9todos de pagamento mais utilizados.</p> </li> <li> <p>Carga (Load): Insere os dados transformados em uma tabela chamada <code>sales_consolidated</code>, localizada em um banco de dados PostgreSQL hospedado na nuvem (Render). A tabela \u00e9 recriada a cada execu\u00e7\u00e3o, garantindo que os dados mais recentes estejam sempre dispon\u00edveis.</p> </li> </ol>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Python 3.12.5: Vers\u00e3o definida e gerenciada com <code>pyenv</code>.</li> <li>Poetry: Gerenciamento de depend\u00eancias e ambiente virtual.</li> <li>Pandas: Manipula\u00e7\u00e3o e processamento de dados.</li> <li>Pandera: Valida\u00e7\u00e3o de esquema para garantir a integridade dos dados.</li> <li>SQLAlchemy: Interface para conex\u00e3o com o banco de dados PostgreSQL.</li> <li>Loguru: Registro de logs de execu\u00e7\u00e3o e de erro para facilitar o monitoramento e a depura\u00e7\u00e3o.</li> <li>Render: Plataforma de hospedagem do banco de dados PostgreSQL, com uma camada gratuita para at\u00e9 certo volume de dados.</li> </ul>"},{"location":"#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Para executar este projeto, voc\u00ea precisar\u00e1 dos seguintes pr\u00e9-requisitos:</p> <ul> <li>Python 3.12.5: Instale e gerencie essa vers\u00e3o com o <code>pyenv</code>.</li> <li>Poetry: Gerenciador de depend\u00eancias e ambientes virtuais.</li> <li>Banco de Dados PostgreSQL: Um banco de dados PostgreSQL configurado na plataforma Render para armazenar os dados processados.</li> </ul>"},{"location":"#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<p>Aqui est\u00e1 uma vis\u00e3o geral da organiza\u00e7\u00e3o do projeto:</p> <pre><code>.\n\u251c\u2500\u2500 app/                 # C\u00f3digo principal da pipeline ETL\n\u251c\u2500\u2500 classes/             # Classes de suporte, incluindo a classe DataExtractor\n\u251c\u2500\u2500 data/                # Diret\u00f3rio para dados de entrada e sa\u00edda\n\u251c\u2500\u2500 decorators/          # Decoradores para logs e medi\u00e7\u00e3o de tempo de execu\u00e7\u00e3o\n\u251c\u2500\u2500 documentation/       # Documenta\u00e7\u00e3o do projeto (configurada para MkDocs)\n\u251c\u2500\u2500 funcs/               # Fun\u00e7\u00f5es auxiliares\n\u251c\u2500\u2500 tests/               # Testes automatizados do projeto\n\u251c\u2500\u2500 .env                 # Arquivo de vari\u00e1veis de ambiente (configura\u00e7\u00e3o do banco de dados)\n\u251c\u2500\u2500 .gitignore           # Arquivo para ignorar arquivos e diret\u00f3rios no Git\n\u251c\u2500\u2500 .python-version      # Vers\u00e3o Python especificada para pyenv (3.12.5)\n\u251c\u2500\u2500 app.log              # Arquivo de log gerado pela execu\u00e7\u00e3o do projeto\n\u251c\u2500\u2500 poetry.lock          # Arquivo de bloqueio de depend\u00eancias gerado pelo Poetry\n\u251c\u2500\u2500 pyproject.toml       # Arquivo de configura\u00e7\u00e3o do Poetry e depend\u00eancias\n\u2514\u2500\u2500 README.md            # Arquivo de documenta\u00e7\u00e3o inicial do projeto\n</code></pre>"},{"location":"#descricao-dos-diretorios-e-arquivos-principais","title":"Descri\u00e7\u00e3o dos Diret\u00f3rios e Arquivos Principais","text":"<ul> <li>app/: Cont\u00e9m o c\u00f3digo principal da pipeline ETL que executa as etapas de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga.</li> <li>classes/: Inclui classes auxiliares, como <code>DataExtractor</code>, para estruturar a extra\u00e7\u00e3o de dados de diferentes formatos.</li> <li>data/: Diret\u00f3rio para armazenamento de dados de entrada e processamento intermedi\u00e1rio.</li> <li>decorators/: Fun\u00e7\u00f5es decoradoras que adicionam funcionalidades de log e medi\u00e7\u00e3o de tempo \u00e0s fun\u00e7\u00f5es principais.</li> <li>documentation/: Diret\u00f3rio onde est\u00e3o armazenados os arquivos de documenta\u00e7\u00e3o para MkDocs, gerando a documenta\u00e7\u00e3o HTML do projeto.</li> <li>funcs/: Armazena fun\u00e7\u00f5es auxiliares, que s\u00e3o utilizadas para suportar o fluxo principal da pipeline.</li> <li>tests/: Cont\u00e9m testes automatizados para verificar a integridade das fun\u00e7\u00f5es e classes do projeto.</li> <li>.env: Arquivo de configura\u00e7\u00e3o para vari\u00e1veis de ambiente, onde s\u00e3o definidas as credenciais do banco de dados PostgreSQL.</li> <li>app.log: Arquivo de log gerado durante a execu\u00e7\u00e3o do projeto, \u00fatil para depura\u00e7\u00e3o e auditoria de processos.</li> <li>poetry.lock e pyproject.toml: Arquivos do Poetry para o gerenciamento de depend\u00eancias e configura\u00e7\u00e3o do ambiente virtual.</li> </ul>"},{"location":"#fluxo-da-pipeline-etl","title":"Fluxo da Pipeline ETL","text":"<p>Abaixo est\u00e1 uma vis\u00e3o geral do fluxo de dados dentro da pipeline ETL:</p> <pre><code>flowchart LR\n    A([.json]) --&gt; B[Extract]\n    C([.parquet]) --&gt; B\n    D([.csv]) --&gt; B\n    B --&gt;|Dados Consolidados| E[Transform]\n    E --&gt;|Dados Transformados| F[Load]\n    F --&gt; G[(Cloud PostgreSQL DB)]</code></pre>"},{"location":"#como-executar-o-projeto","title":"Como Executar o Projeto","text":"<ol> <li> <p>Clone este reposit\u00f3rio executando:</p> <pre><code>git clone git@github.com:BrunoChiconato/workshop_estruturando_projeto_dados.git\n</code></pre> </li> <li> <p>Verifique se o <code>pyenv</code> est\u00e1 instalado. A vers\u00e3o Python ser\u00e1 automaticamente configurada para 3.12.5 ao entrar no diret\u00f3rio do projeto, conforme especificado no arquivo <code>.python-version</code>.</p> </li> <li> <p>Instale o <code>Poetry</code> e todas as depend\u00eancias do projeto com o comando:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Gere arquivos de exemplo para a pipeline executando:</p> <pre><code>poetry run task gen_data\n</code></pre> </li> <li> <p>Execute o c\u00f3digo principal localizado em <code>pipeline.py</code> utilizando:</p> <pre><code>poetry run task main\n</code></pre> </li> <li> <p>Liste os outros comandos dispon\u00edveis neste projeto com:</p> <pre><code>poetry run task --list\n</code></pre> </li> </ol>"}]}