{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bem-vindo","text":"<p>Este projeto implementa uma pipeline de dados automatizada para extrair, transformar e carregar (ETL) dados de vendas fict\u00edcias de diversos formatos de arquivo para um banco de dados PostgreSQL na nuvem. A pipeline foi projetada para consolidar dados de m\u00faltiplas fontes em um \u00fanico DataFrame, aplicar transforma\u00e7\u00f5es espec\u00edficas e armazen\u00e1-los em um reposit\u00f3rio central para facilitar a an\u00e1lise e a consulta.</p>"},{"location":"#objetivo-do-projeto","title":"Objetivo do Projeto","text":"<p>A pipeline ETL realiza as seguintes opera\u00e7\u00f5es principais:</p> <ol> <li> <p>Extra\u00e7\u00e3o (Extract): Extrai dados de arquivos <code>.csv</code>, <code>.json</code> e <code>.parquet</code> localizados em um diret\u00f3rio espec\u00edfico. Cada arquivo cont\u00e9m informa\u00e7\u00f5es de vendas, como identificadores de pedidos, clientes, produtos, m\u00e9todo de pagamento, quantidade e pre\u00e7o.</p> </li> <li> <p>Transforma\u00e7\u00e3o (Transform): Processa os dados consolidados para calcular o valor total das vendas, agrupando-os por m\u00e9todo de pagamento. Esse processo resulta em um resumo que permite uma an\u00e1lise mais f\u00e1cil e r\u00e1pida dos m\u00e9todos de pagamento mais utilizados.</p> </li> <li> <p>Carga (Load): Insere os dados transformados em uma tabela chamada <code>sales_consolidated</code>, localizada em um banco de dados PostgreSQL hospedado na nuvem (Render). A tabela \u00e9 recriada a cada execu\u00e7\u00e3o, garantindo que os dados mais recentes estejam sempre dispon\u00edveis.</p> </li> </ol>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Python 3.12.5: Vers\u00e3o definida e gerenciada com <code>pyenv</code>.</li> <li>Poetry: Gerenciamento de depend\u00eancias e ambiente virtual.</li> <li>Pandas: Manipula\u00e7\u00e3o e processamento de dados.</li> <li>Pandera: Valida\u00e7\u00e3o de esquema para garantir a integridade dos dados.</li> <li>SQLAlchemy: Interface para conex\u00e3o com o banco de dados PostgreSQL.</li> <li>Loguru: Registro de logs de execu\u00e7\u00e3o e de erro para facilitar o monitoramento e a depura\u00e7\u00e3o.</li> <li>Render: Plataforma de hospedagem do banco de dados PostgreSQL, com uma camada gratuita para at\u00e9 certo volume de dados.</li> </ul>"},{"location":"#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Para executar este projeto, voc\u00ea precisar\u00e1 dos seguintes pr\u00e9-requisitos:</p> <ul> <li>Python 3.12.5: Instale e gerencie essa vers\u00e3o com o <code>pyenv</code>.</li> <li>Poetry: Gerenciador de depend\u00eancias e ambientes virtuais.</li> <li>Banco de Dados PostgreSQL: Um banco de dados PostgreSQL configurado na plataforma Render para armazenar os dados processados.</li> </ul>"},{"location":"#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<p>Aqui est\u00e1 uma vis\u00e3o geral da organiza\u00e7\u00e3o do projeto:</p> <pre><code>.\n\u251c\u2500\u2500 app/                 # C\u00f3digo principal da pipeline ETL\n\u251c\u2500\u2500 classes/             # Classes de suporte, incluindo a classe DataExtractor\n\u251c\u2500\u2500 data/                # Diret\u00f3rio para dados de entrada e sa\u00edda\n\u251c\u2500\u2500 decorators/          # Decoradores para logs e medi\u00e7\u00e3o de tempo de execu\u00e7\u00e3o\n\u251c\u2500\u2500 documentation/       # Documenta\u00e7\u00e3o do projeto (configurada para MkDocs)\n\u251c\u2500\u2500 funcs/               # Fun\u00e7\u00f5es auxiliares\n\u251c\u2500\u2500 tests/               # Testes automatizados do projeto\n\u251c\u2500\u2500 .env                 # Arquivo de vari\u00e1veis de ambiente (configura\u00e7\u00e3o do banco de dados)\n\u251c\u2500\u2500 .gitignore           # Arquivo para ignorar arquivos e diret\u00f3rios no Git\n\u251c\u2500\u2500 .python-version      # Vers\u00e3o Python especificada para pyenv (3.12.5)\n\u251c\u2500\u2500 app.log              # Arquivo de log gerado pela execu\u00e7\u00e3o do projeto\n\u251c\u2500\u2500 poetry.lock          # Arquivo de bloqueio de depend\u00eancias gerado pelo Poetry\n\u251c\u2500\u2500 pyproject.toml       # Arquivo de configura\u00e7\u00e3o do Poetry e depend\u00eancias\n\u2514\u2500\u2500 README.md            # Arquivo de documenta\u00e7\u00e3o inicial do projeto\n</code></pre>"},{"location":"#descricao-dos-diretorios-e-arquivos-principais","title":"Descri\u00e7\u00e3o dos Diret\u00f3rios e Arquivos Principais","text":"<ul> <li>app/: Cont\u00e9m o c\u00f3digo principal da pipeline ETL que executa as etapas de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga.</li> <li>classes/: Inclui classes auxiliares, como <code>DataExtractor</code>, para estruturar a extra\u00e7\u00e3o de dados de diferentes formatos.</li> <li>data/: Diret\u00f3rio para armazenamento de dados de entrada e processamento intermedi\u00e1rio.</li> <li>decorators/: Fun\u00e7\u00f5es decoradoras que adicionam funcionalidades de log e medi\u00e7\u00e3o de tempo \u00e0s fun\u00e7\u00f5es principais.</li> <li>documentation/: Diret\u00f3rio onde est\u00e3o armazenados os arquivos de documenta\u00e7\u00e3o para MkDocs, gerando a documenta\u00e7\u00e3o HTML do projeto.</li> <li>funcs/: Armazena fun\u00e7\u00f5es auxiliares, que s\u00e3o utilizadas para suportar o fluxo principal da pipeline.</li> <li>tests/: Cont\u00e9m testes automatizados para verificar a integridade das fun\u00e7\u00f5es e classes do projeto.</li> <li>.env: Arquivo de configura\u00e7\u00e3o para vari\u00e1veis de ambiente, onde s\u00e3o definidas as credenciais do banco de dados PostgreSQL.</li> <li>app.log: Arquivo de log gerado durante a execu\u00e7\u00e3o do projeto, \u00fatil para depura\u00e7\u00e3o e auditoria de processos.</li> <li>poetry.lock e pyproject.toml: Arquivos do Poetry para o gerenciamento de depend\u00eancias e configura\u00e7\u00e3o do ambiente virtual.</li> </ul>"},{"location":"#fluxo-da-pipeline-etl","title":"Fluxo da Pipeline ETL","text":"<p>Abaixo est\u00e1 uma vis\u00e3o geral do fluxo de dados dentro da pipeline ETL:</p> <pre><code>flowchart LR\n    A([.json]) --&gt; B[Extract]\n    C([.parquet]) --&gt; B\n    D([.csv]) --&gt; B\n    B --&gt;|Dados Consolidados| E[Transform]\n    E --&gt;|Dados Transformados| F[Load]\n    F --&gt; G[(Cloud PostgreSQL DB)]</code></pre>"},{"location":"#como-executar-o-projeto","title":"Como Executar o Projeto","text":"<ol> <li> <p>Clone este reposit\u00f3rio executando:</p> <pre><code>git clone git@github.com:BrunoChiconato/workshop_estruturando_projeto_dados.git\n</code></pre> </li> <li> <p>Verifique se o <code>pyenv</code> est\u00e1 instalado. A vers\u00e3o Python ser\u00e1 automaticamente configurada para 3.12.5 ao entrar no diret\u00f3rio do projeto, conforme especificado no arquivo <code>.python-version</code>.</p> </li> <li> <p>Instale o <code>Poetry</code> e todas as depend\u00eancias do projeto com o comando:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Gere arquivos de exemplo para a pipeline executando:</p> <pre><code>poetry run task gen_data\n</code></pre> </li> <li> <p>Execute o c\u00f3digo principal localizado em <code>pipeline.py</code> utilizando:</p> <pre><code>poetry run task main\n</code></pre> </li> <li> <p>Liste os outros comandos dispon\u00edveis neste projeto com:</p> <pre><code>poetry run task --list\n</code></pre> </li> </ol>"},{"location":"extract/","title":"Extra\u00e7\u00e3o dos Dados","text":""},{"location":"extract/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>Esta se\u00e7\u00e3o descreve o c\u00f3digo em Python localizado em <code>funcs/extract.py</code>, que extrai dados de diferentes tipos de arquivos, consolida-os em um DataFrame e retorna o DataFrame consolidado. Para alcan\u00e7ar esse objetivo, \u00e9 definida a fun\u00e7\u00e3o <code>extract_and_consolidate</code>.</p> <p>A fun\u00e7\u00e3o <code>extract_and_consolidate</code> utiliza a biblioteca <code>pandas</code> para a extra\u00e7\u00e3o e consolida\u00e7\u00e3o dos dados. Ela faz uso da classe <code>DataExtractor</code> (descrita em detalhes na se\u00e7\u00e3o Classes) e aplica dois decoradores: um para registrar logs e outro para calcular o tempo total de execu\u00e7\u00e3o da fun\u00e7\u00e3o, ambos explicados na se\u00e7\u00e3o Decoradores desta documenta\u00e7\u00e3o.</p> <p>A fun\u00e7\u00e3o recebe como par\u00e2metro o caminho do diret\u00f3rio onde os dados de entrada est\u00e3o armazenados. Em seguida, cria uma inst\u00e2ncia da classe <code>DataExtractor</code> e utiliza seus m\u00e9todos para ler dados em diferentes formatos, armazenando cada conjunto de dados em um DataFrame espec\u00edfico. Por fim, a fun\u00e7\u00e3o concatena todos esses DataFrames e retorna o resultado consolidado ao usu\u00e1rio.</p>"},{"location":"extract/#funcao-extract_and_consolidate","title":"Fun\u00e7\u00e3o <code>extract_and_consolidate</code>","text":"<p>Extrai e consolida dados de arquivos CSV, JSON e Parquet em um \u00fanico DataFrame.</p> <p>Esta fun\u00e7\u00e3o cria uma inst\u00e2ncia da classe DataExtractor para ler dados de diferentes formatos de arquivos (CSV, JSON e Parquet) presentes em um diret\u00f3rio espec\u00edfico. Ap\u00f3s a extra\u00e7\u00e3o, os dados s\u00e3o concatenados em um \u00fanico DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Caminho do diret\u00f3rio onde os arquivos CSV, JSON e Parquet est\u00e3o localizados.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame contendo os dados consolidados de todos os arquivos extra\u00eddos.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Se algum dos arquivos CSV, JSON ou Parquet n\u00e3o for encontrado no diret\u00f3rio especificado.</p> Source code in <code>funcs\\extract.py</code> <pre><code>@time_decorador\n@log_decorator\ndef extract_and_consolidate(data_path: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Extrai e consolida dados de arquivos CSV, JSON e Parquet em um \u00fanico DataFrame.\n\n    Esta fun\u00e7\u00e3o cria uma inst\u00e2ncia da classe DataExtractor para ler dados de diferentes formatos\n    de arquivos (CSV, JSON e Parquet) presentes em um diret\u00f3rio espec\u00edfico. Ap\u00f3s a extra\u00e7\u00e3o, os\n    dados s\u00e3o concatenados em um \u00fanico DataFrame.\n\n    Parameters:\n        data_path (str): Caminho do diret\u00f3rio onde os arquivos CSV, JSON e Parquet est\u00e3o localizados.\n\n    Returns:\n        pd.DataFrame: DataFrame contendo os dados consolidados de todos os arquivos extra\u00eddos.\n\n    Raises:\n        FileNotFoundError: Se algum dos arquivos CSV, JSON ou Parquet n\u00e3o for encontrado no diret\u00f3rio especificado.\n    \"\"\"\n    extractor = DataExtractor()\n\n    csv_data: pd.DataFrame = extractor.read_csv_data(input_path = data_path)\n    json_data: pd.DataFrame = extractor.read_json_data(input_path = data_path)\n    parquet_data: pd.DataFrame = extractor.read_parquet_data(input_path = data_path)\n\n    consolidate_data: pd.DataFrame = pd.concat([csv_data, json_data, parquet_data], ignore_index=True)\n    return consolidate_data\n</code></pre>"},{"location":"extract/#teste-unitario","title":"Teste unit\u00e1rio","text":"<p>Esse teste unit\u00e1rio verifica o comportamento correto da fun\u00e7\u00e3o <code>extract_and_consolidate</code> utilizando mocks para simular as opera\u00e7\u00f5es da classe <code>DataExtractor</code>. Isso permite validar a fun\u00e7\u00e3o sem realmente acessar o sistema de arquivos.</p>"},{"location":"extract/#aspectos-validados","title":"Aspectos validados","text":"<ol> <li>Valida\u00e7\u00e3o do caminho de entrada: O teste assegura que o m\u00e9todo <code>validate_input_path</code> aceita o caminho dos dados fornecido.</li> <li>Leitura de dados simulados:<ul> <li>S\u00e3o criados mocks para os m\u00e9todos <code>read_csv_data</code>, <code>read_json_data</code> e <code>read_parquet_data</code>, que retornam DataFrames pr\u00e9-definidos para simular dados CSV, JSON e Parquet.</li> <li>Esses mocks garantem que o teste \u00e9 isolado do sistema de arquivos real.</li> </ul> </li> <li>Consolida\u00e7\u00e3o dos dados:<ul> <li>O teste chama a fun\u00e7\u00e3o <code>extract_and_consolidate</code> e compara o DataFrame resultante com um DataFrame esperado que cont\u00e9m a consolida\u00e7\u00e3o dos dados simulados.</li> <li>A compara\u00e7\u00e3o \u00e9 feita com <code>pd.testing.assert_frame_equal</code>, que verifica se o DataFrame resultante \u00e9 id\u00eantico ao esperado em estrutura e valores.</li> </ul> </li> <li>Verifica\u00e7\u00e3o de chamadas:<ul> <li>O teste verifica que cada m\u00e9todo de leitura (<code>read_csv_data</code>, <code>read_json_data</code>, <code>read_parquet_data</code>) foi chamado exatamente uma vez com o caminho correto, usando <code>assert_called_once_with</code>.</li> </ul> </li> <li>Asser\u00e7\u00e3o final: Caso a sa\u00edda da fun\u00e7\u00e3o n\u00e3o corresponda ao DataFrame esperado, um erro de asser\u00e7\u00e3o \u00e9 levantado, indicando uma falha no comportamento esperado da fun\u00e7\u00e3o.</li> </ol>"},{"location":"extract/#funcao-test_extract_and_consolidate","title":"Fun\u00e7\u00e3o <code>test_extract_and_consolidate</code>","text":"<p>Testa a fun\u00e7\u00e3o <code>extract_and_consolidate</code> para verificar a consolida\u00e7\u00e3o de dados de m\u00faltiplas fontes (CSV, JSON e Parquet) em um \u00fanico DataFrame.</p> <p>Este teste simula os m\u00e9todos da classe <code>DataExtractor</code> usando mocks, evitando o acesso ao sistema de arquivos e garantindo que a fun\u00e7\u00e3o <code>extract_and_consolidate</code> funcione corretamente. S\u00e3o validados os seguintes aspectos:</p> <ul> <li>O m\u00e9todo <code>validate_input_path</code> aceita o caminho dos dados fornecido.</li> <li>Os m\u00e9todos <code>read_csv_data</code>, <code>read_json_data</code> e <code>read_parquet_data</code> retornam DataFrames simulados   para os dados CSV, JSON e Parquet, respectivamente.</li> <li>A fun\u00e7\u00e3o <code>extract_and_consolidate</code> retorna um DataFrame consolidado com a estrutura e os valores esperados.</li> <li>Cada m\u00e9todo de leitura \u00e9 chamado exatamente uma vez com o caminho correto.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>MockDataExtractor</code> <code>MagicMock</code> <p>Mock da classe <code>DataExtractor</code> para simular a leitura de dados                            e evitar acessos reais ao sistema de arquivos.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>Se a sa\u00edda da fun\u00e7\u00e3o n\u00e3o corresponder ao DataFrame esperado.</p> Source code in <code>tests\\test_extract.py</code> <pre><code>@patch('funcs.extract.DataExtractor')\ndef test_extract_and_consolidate(MockDataExtractor):\n    \"\"\"\n    Testa a fun\u00e7\u00e3o `extract_and_consolidate` para verificar a consolida\u00e7\u00e3o de dados de m\u00faltiplas\n    fontes (CSV, JSON e Parquet) em um \u00fanico DataFrame.\n\n    Este teste simula os m\u00e9todos da classe `DataExtractor` usando mocks, evitando o acesso\n    ao sistema de arquivos e garantindo que a fun\u00e7\u00e3o `extract_and_consolidate` funcione corretamente.\n    S\u00e3o validados os seguintes aspectos:\n\n    - O m\u00e9todo `validate_input_path` aceita o caminho dos dados fornecido.\n    - Os m\u00e9todos `read_csv_data`, `read_json_data` e `read_parquet_data` retornam DataFrames simulados\n      para os dados CSV, JSON e Parquet, respectivamente.\n    - A fun\u00e7\u00e3o `extract_and_consolidate` retorna um DataFrame consolidado com a estrutura e os valores esperados.\n    - Cada m\u00e9todo de leitura \u00e9 chamado exatamente uma vez com o caminho correto.\n\n    Parameters:\n        MockDataExtractor (MagicMock): Mock da classe `DataExtractor` para simular a leitura de dados\n                                       e evitar acessos reais ao sistema de arquivos.\n\n    Raises:\n        AssertionError: Se a sa\u00edda da fun\u00e7\u00e3o n\u00e3o corresponder ao DataFrame esperado.\n    \"\"\"\n    mock_extractor = MockDataExtractor.return_value\n\n    mock_extractor.validate_input_path.side_effect = lambda path: path\n\n    mock_extractor.read_csv_data.return_value = pd.DataFrame({\n        'order_id': [1, 2],\n        'customer_id': [202, 448],\n        'order_date': ['2023-01-01', '2023-01-02'],\n        'product_id': [1484, 1027],\n        'quantity': [5, 4],\n        'price': [99.68, 20.8],\n        'payment_method': ['Cash', 'Debit Card'],\n        'store_location': ['Los Angeles', 'Houston']\n    })\n    mock_extractor.read_json_data.return_value = pd.DataFrame({\n        'order_id': [3],\n        'customer_id': [370],\n        'order_date': ['2023-01-03'],\n        'product_id': [1713],\n        'quantity': [6],\n        'price': [362.8],\n        'payment_method': ['Credit Card'],\n        'store_location': ['New York']\n    })\n    mock_extractor.read_parquet_data.return_value = pd.DataFrame({\n        'order_id': [4],\n        'customer_id': [206],\n        'order_date': ['2023-01-04'],\n        'product_id': [1038],\n        'quantity': [7],\n        'price': [214.82],\n        'payment_method': ['Credit Card'],\n        'store_location': ['Houston']\n    })\n\n    data_path = 'fake_path'\n\n    result = extract_and_consolidate(data_path)\n\n    mock_extractor.read_csv_data.assert_called_once_with(input_path=data_path)\n    mock_extractor.read_json_data.assert_called_once_with(input_path=data_path)\n    mock_extractor.read_parquet_data.assert_called_once_with(input_path=data_path)\n\n    expected_data = pd.DataFrame({\n        'order_id': [1, 2, 3, 4],\n        'customer_id': [202, 448, 370, 206],\n        'order_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04'],\n        'product_id': [1484, 1027, 1713, 1038],\n        'quantity': [5, 4, 6, 7],\n        'price': [99.68, 20.8, 362.8, 214.82],\n        'payment_method': ['Cash', 'Debit Card', 'Credit Card', 'Credit Card'],\n        'store_location': ['Los Angeles', 'Houston', 'New York', 'Houston']\n    })\n\n    pd.testing.assert_frame_equal(result, expected_data)\n</code></pre>"},{"location":"generate_data/","title":"Gera\u00e7\u00e3o dos Dados","text":""},{"location":"generate_data/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>A presente se\u00e7\u00e3o inclui um c\u00f3digo em Python, localizado em <code>funcs/generate_data.py</code>, que gera dados fict\u00edcios de vendas para diversas finalidades. A fun\u00e7\u00e3o principal, <code>generate_data</code>, utiliza as bibliotecas <code>os</code>, <code>pandas</code> e <code>numpy</code> para criar dados simulados e salv\u00e1-los em formatos variados, incluindo <code>.csv</code>, <code>.json</code> e <code>.parquet</code>.</p> <p>Abaixo est\u00e1 uma descri\u00e7\u00e3o detalhada do funcionamento da fun\u00e7\u00e3o <code>generate_data</code>:</p> <ol> <li> <p>Par\u00e2metro de caminho: A fun\u00e7\u00e3o recebe uma string que especifica o caminho onde os dados gerados ser\u00e3o salvos. Esse caminho pode ser personalizado pelo usu\u00e1rio.</p> </li> <li> <p>Defini\u00e7\u00e3o do tamanho dos dados: O n\u00famero de linhas do conjunto de dados \u00e9 definido na fun\u00e7\u00e3o. Neste projeto, o valor padr\u00e3o \u00e9 <code>n = 1000</code>, mas pode ser ajustado conforme a necessidade.</p> </li> <li> <p>Cria\u00e7\u00e3o do DataFrame: Utilizando a biblioteca <code>pandas</code>, a fun\u00e7\u00e3o cria um DataFrame e define colunas que representam um relat\u00f3rio de vendas fict\u00edcio. Para popular essas colunas, s\u00e3o utilizados arrays gerados com a biblioteca <code>numpy</code>, simulando dados como identificadores de vendas, valores, datas e outros detalhes relevantes.</p> </li> <li> <p>Verifica\u00e7\u00e3o e cria\u00e7\u00e3o do caminho: A fun\u00e7\u00e3o verifica se o caminho fornecido existe. Caso o caminho n\u00e3o exista, ele \u00e9 criado automaticamente para garantir que os arquivos possam ser salvos.</p> </li> <li> <p>Salvamento dos arquivos: O DataFrame gerado \u00e9 salvo nos tr\u00eas formatos de arquivo especificados (<code>.csv</code>, <code>.json</code> e <code>.parquet</code>). Ap\u00f3s o salvamento, uma mensagem de sucesso \u00e9 exibida, confirmando que os arquivos foram gerados com sucesso.</p> </li> </ol>"},{"location":"generate_data/#funcao-generate_data","title":"Fun\u00e7\u00e3o <code>generate_data</code>","text":"<p>Gera dados fict\u00edcios de vendas e salva em CSV, Parquet e JSON.</p> <p>Esta fun\u00e7\u00e3o cria um conjunto de dados fict\u00edcios com as seguintes colunas:</p> <ul> <li>order_id (int)</li> <li>customer_id (int)</li> <li>order_date (date)</li> <li>product_id (int)</li> <li>quantity (int)</li> <li>price (float)</li> <li>payment_method (string)</li> <li>store_location (string)</li> </ul> <p>A fun\u00e7\u00e3o verifica se o diret\u00f3rio especificado pelo argumento 'path' existe. Caso o diret\u00f3rio n\u00e3o exista, ele ser\u00e1 criado. Em seguida, os dados s\u00e3o gerados e salvos nos formatos 'csv', 'parquet' e 'json'.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Caminho da pasta onde os arquivos ser\u00e3o salvos.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>funcs\\generate_data.py</code> <pre><code>def generate_data(path: str) -&gt; None:\n    \"\"\"\n    Gera dados fict\u00edcios de vendas e salva em CSV, Parquet e JSON.\n\n    Esta fun\u00e7\u00e3o cria um conjunto de dados fict\u00edcios com as seguintes colunas:\n\n    - order_id (int)\n    - customer_id (int)\n    - order_date (date)\n    - product_id (int)\n    - quantity (int)\n    - price (float)\n    - payment_method (string)\n    - store_location (string)\n\n    A fun\u00e7\u00e3o verifica se o diret\u00f3rio especificado pelo argumento 'path' existe.\n    Caso o diret\u00f3rio n\u00e3o exista, ele ser\u00e1 criado. Em seguida, os dados s\u00e3o gerados\n    e salvos nos formatos 'csv', 'parquet' e 'json'.\n\n    Parameters:\n        path (str): Caminho da pasta onde os arquivos ser\u00e3o salvos.\n\n    Returns:\n        None\n    \"\"\"\n    n: int = 1000\n    data: pd.DataFrame = pd.DataFrame({\n        'order_id': np.arange(1, n+1),\n        'customer_id': np.random.randint(100, 500, size=n),\n        'order_date': pd.date_range('2023-01-01', periods=n, freq='D'),\n        'product_id': np.random.randint(1000, 2000, size=n),\n        'quantity': np.random.randint(1, 10, size=n),\n        'price': np.round(np.random.uniform(10, 500, size=n), 2),\n        'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'Cash', 'PayPal'], size=n),\n        'store_location': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], size=n)\n    })\n\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)\n\n    data.to_csv(os.path.join(path, 'vendas_ficticias.csv'), index=False)\n    data.to_parquet(os.path.join(path,'vendas_ficticias.parquet'), index=False)\n    data.to_json(os.path.join(path,'vendas_ficticias.json'), orient='records', lines=True)\n\n    print('Arquivos criados com sucesso!')\n</code></pre>"},{"location":"transform/","title":"Transforma\u00e7\u00e3o dos Dados","text":""},{"location":"transform/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>Esta se\u00e7\u00e3o descreve a fun\u00e7\u00e3o <code>transform_data</code>, localizada em <code>funcs/transform.py</code>. Essa fun\u00e7\u00e3o recebe um <code>DataFrame</code> de entrada e aplica uma s\u00e9rie de verifica\u00e7\u00f5es e opera\u00e7\u00f5es de transforma\u00e7\u00e3o, com o objetivo de consolidar os valores da coluna <code>price</code> por m\u00e9todo de pagamento (<code>payment_method</code>). O resultado final \u00e9 um novo <code>DataFrame</code> com cada m\u00e9todo de pagamento \u00fanico e a soma dos pre\u00e7os correspondentes.</p> <p>A fun\u00e7\u00e3o \u00e9 decorada com dois decoradores: - <code>log_decorator</code>: Registra logs da execu\u00e7\u00e3o, permitindo rastrear o processamento dos dados. - <code>time_decorador</code>: Mede o tempo de execu\u00e7\u00e3o da fun\u00e7\u00e3o, facilitando o monitoramento de performance dentro da pipeline.</p>"},{"location":"transform/#funcionamento","title":"Funcionamento","text":"<p>O funcionamento da fun\u00e7\u00e3o <code>transform_data</code> pode ser dividido nas seguintes etapas:</p> <ol> <li> <p>Verifica\u00e7\u00e3o das colunas necess\u00e1rias: A fun\u00e7\u00e3o come\u00e7a verificando se as colunas <code>payment_method</code> e <code>price</code> est\u00e3o presentes no <code>DataFrame</code> de entrada. Caso uma dessas colunas esteja ausente, um erro do tipo <code>KeyError</code> \u00e9 levantado, informando a aus\u00eancia da coluna necess\u00e1ria. Esse mecanismo de verifica\u00e7\u00e3o garante que a fun\u00e7\u00e3o s\u00f3 prossiga se os dados m\u00ednimos exigidos estiverem presentes.</p> </li> <li> <p>Convers\u00e3o de valores para num\u00e9rico: Em seguida, a fun\u00e7\u00e3o tenta converter os valores da coluna <code>price</code> para um tipo num\u00e9rico. Se houver valores n\u00e3o num\u00e9ricos, eles s\u00e3o convertidos para <code>NaN</code>. Essa etapa \u00e9 importante para garantir que apenas valores v\u00e1lidos sejam considerados na agrega\u00e7\u00e3o dos dados.</p> </li> <li> <p>Valida\u00e7\u00e3o de dados num\u00e9ricos: A fun\u00e7\u00e3o verifica se todos os valores na coluna <code>price</code> s\u00e3o inv\u00e1lidos (<code>NaN</code>). Caso todos os valores sejam inv\u00e1lidos, a fun\u00e7\u00e3o retorna um <code>DataFrame</code> vazio e exibe uma mensagem de erro, informando que n\u00e3o h\u00e1 dados v\u00e1lidos para o c\u00e1lculo.</p> </li> <li> <p>Agrupamento e soma dos valores: Ap\u00f3s a valida\u00e7\u00e3o, a fun\u00e7\u00e3o realiza o agrupamento dos dados pela coluna <code>payment_method</code>, somando os valores correspondentes de <code>price</code> para cada m\u00e9todo de pagamento. O resultado \u00e9 um novo <code>DataFrame</code> com as colunas <code>payment_method</code> e <code>price</code>, onde cada linha representa um m\u00e9todo de pagamento \u00fanico e a soma total de <code>price</code> associada a ele.</p> </li> <li> <p>Tratamento de exce\u00e7\u00f5es: Caso ocorra qualquer erro inesperado durante a execu\u00e7\u00e3o da fun\u00e7\u00e3o, uma mensagem de erro \u00e9 exibida e um <code>DataFrame</code> vazio \u00e9 retornado. Esse tratamento garante a robustez da fun\u00e7\u00e3o, evitando falhas cr\u00edticas na pipeline ETL.</p> </li> </ol>"},{"location":"transform/#funcao-transform_data","title":"Fun\u00e7\u00e3o <code>transform_data</code>","text":"<p>Transforma um DataFrame agrupando os valores de 'price' por 'payment_method'.</p> <p>Esta fun\u00e7\u00e3o verifica se as colunas 'payment_method' e 'price' est\u00e3o presentes no DataFrame fornecido. Em seguida, realiza um agrupamento dos dados, somando os valores da coluna 'price' para cada m\u00e9todo de pagamento. Retorna um novo DataFrame com duas colunas: 'payment_method' e 'price', onde 'price' representa a soma total para cada m\u00e9todo de pagamento.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <p>pd.DataFrame DataFrame de entrada que deve conter as colunas 'payment_method' e 'price'.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame DataFrame transformado com as colunas: - 'payment_method': M\u00e9todos de pagamento \u00fanicos. - 'price': Soma dos valores da coluna 'price' para cada m\u00e9todo de pagamento.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>Se a coluna 'payment_method' ou 'price' n\u00e3o estiver presente no DataFrame.</p> Exception <p>Se ocorrer qualquer outro erro durante a execu\u00e7\u00e3o da fun\u00e7\u00e3o. Neste caso, uma mensagem de erro ser\u00e1 exibida e um DataFrame vazio ser\u00e1 retornado.</p> Source code in <code>funcs\\transform.py</code> <pre><code>@time_decorador\n@log_decorator\ndef transform_data(data: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforma um DataFrame agrupando os valores de 'price' por 'payment_method'.\n\n    Esta fun\u00e7\u00e3o verifica se as colunas 'payment_method' e 'price' est\u00e3o presentes no\n    DataFrame fornecido. Em seguida, realiza um agrupamento dos dados, somando os valores\n    da coluna 'price' para cada m\u00e9todo de pagamento. Retorna um novo DataFrame com duas\n    colunas: 'payment_method' e 'price', onde 'price' representa a soma total para cada\n    m\u00e9todo de pagamento.\n\n    Parameters:\n        data : pd.DataFrame\n            DataFrame de entrada que deve conter as colunas 'payment_method' e 'price'.\n\n    Returns:\n        pd.DataFrame\n            DataFrame transformado com as colunas:\n            - 'payment_method': M\u00e9todos de pagamento \u00fanicos.\n            - 'price': Soma dos valores da coluna 'price' para cada m\u00e9todo de pagamento.\n\n    Raises:\n        KeyError: Se a coluna 'payment_method' ou 'price' n\u00e3o estiver presente no DataFrame.\n\n    Exception:\n        Se ocorrer qualquer outro erro durante a execu\u00e7\u00e3o da fun\u00e7\u00e3o. Neste caso, uma\n        mensagem de erro ser\u00e1 exibida e um DataFrame vazio ser\u00e1 retornado.\n    \"\"\"\n    transform_data: pd.DataFrame = pd.DataFrame()\n\n    try:\n        columns = data.columns.str.strip()\n\n        if 'payment_method' not in columns:\n            raise KeyError(\"A coluna 'payment_method' n\u00e3o foi encontrada.\")\n        if 'price' not in columns:\n            raise KeyError(\"A coluna 'price' n\u00e3o foi encontrada.\")\n\n        data['price'] = pd.to_numeric(data['price'], errors='coerce')\n\n        if data['price'].isna().all():\n            print('Erro: Todos os valores de \"price\" s\u00e3o inv\u00e1lidos. Tente novamente.')\n            return transform_data\n\n        transform_data = data.groupby('payment_method')['price'].sum().reset_index()\n        transform_data.columns = ['payment_method', 'price']\n\n        return transform_data\n\n    except Exception as e:\n        print(f'Erro: {e} Tente novamente.')\n        return transform_data\n</code></pre>"},{"location":"transform/#teste-unitario","title":"Teste unit\u00e1rio","text":"<p>Os testes descritos cobrem casos de uso positivo, al\u00e9m de cen\u00e1rios com entradas inv\u00e1lidas e erros esperados, assegurando que a fun\u00e7\u00e3o se comporte conforme o esperado e garanta a estabilidade da pipeline de dados.</p>"},{"location":"transform/#estrutura-dos-testes","title":"Estrutura dos testes","text":"<p>Os testes foram implementados utilizando o <code>pytest</code>, uma biblioteca de testes popular no ecossistema Python. Abaixo est\u00e3o os testes definidos para a fun\u00e7\u00e3o <code>transform_data</code>, localizados no arquivo de testes correspondente.</p>"},{"location":"transform/#testes-implementados","title":"Testes implementados","text":"<ol> <li>Teste de sucesso (<code>test_transform_data_success</code>):<ul> <li>Objetivo: Verificar se a fun\u00e7\u00e3o <code>transform_data</code> consegue agrupar corretamente os dados de entrada v\u00e1lidos, somando os valores da coluna <code>price</code> para cada <code>payment_method</code>.</li> <li>Descri\u00e7\u00e3o: Um <code>DataFrame</code> de entrada \u00e9 passado para a fun\u00e7\u00e3o, contendo m\u00e9todos de pagamento variados com valores num\u00e9ricos. O resultado esperado \u00e9 um novo <code>DataFrame</code> com os m\u00e9todos de pagamento \u00fanicos e a soma correta dos pre\u00e7os.</li> <li>Valida\u00e7\u00e3o: Usa <code>pd.testing.assert_frame_equal</code> para comparar o <code>DataFrame</code> resultante com o esperado, garantindo a exatid\u00e3o do agrupamento e da soma dos valores.</li> <li>Fun\u00e7\u00e3o de teste: </li> </ul> </li> </ol> <p>Testa a fun\u00e7\u00e3o <code>transform_data</code> com um DataFrame v\u00e1lido, verificando se o agrupamento por 'payment_method' \u00e9 realizado corretamente.</p> Source code in <code>tests\\test_transform.py</code> <pre><code>def test_transform_data_success():\n    \"\"\"\n    Testa a fun\u00e7\u00e3o `transform_data` com um DataFrame v\u00e1lido, verificando se\n    o agrupamento por 'payment_method' \u00e9 realizado corretamente.\n    \"\"\"\n    input_data = pd.DataFrame({\n        'payment_method': ['Credit Card', 'Cash', 'Credit Card', 'Debit Card', 'Cash'],\n        'price': [100.0, 50.0, 150.0, 200.0, 50.0]\n    })\n\n    expected_data = pd.DataFrame({\n        'payment_method': ['Cash', 'Credit Card', 'Debit Card'],\n        'price': [100.0, 250.0, 200.0]\n    })\n\n    result = transform_data(input_data)\n\n    pd.testing.assert_frame_equal(result, expected_data)\n</code></pre> <ol> <li>Teste de colunas ausentes (<code>test_transform_data_missing_columns</code>):<ul> <li>Objetivo: Confirmar que a fun\u00e7\u00e3o retorna um <code>DataFrame</code> vazio quando uma das colunas necess\u00e1rias (<code>payment_method</code> ou <code>price</code>) est\u00e1 ausente no <code>DataFrame</code> de entrada.</li> <li>Descri\u00e7\u00e3o: Dois cen\u00e1rios s\u00e3o testados, um em que a coluna <code>payment_method</code> est\u00e1 ausente e outro em que a coluna <code>price</code> est\u00e1 ausente. Em ambos os casos, espera-se que a fun\u00e7\u00e3o retorne um <code>DataFrame</code> vazio, pois esses dados s\u00e3o obrigat\u00f3rios para o c\u00e1lculo.</li> <li>Valida\u00e7\u00e3o: Usa <code>pd.testing.assert_frame_equal</code> para verificar que o resultado \u00e9 um <code>DataFrame</code> vazio, conforme o esperado.</li> <li>Fun\u00e7\u00e3o de teste: </li> </ul> </li> </ol> <p>Testa a fun\u00e7\u00e3o <code>transform_data</code> quando as colunas necess\u00e1rias est\u00e3o ausentes, esperando que um DataFrame vazio seja retornado.</p> Source code in <code>tests\\test_transform.py</code> <pre><code>def test_transform_data_missing_columns():\n    \"\"\"\n    Testa a fun\u00e7\u00e3o `transform_data` quando as colunas necess\u00e1rias est\u00e3o ausentes,\n    esperando que um DataFrame vazio seja retornado.\n    \"\"\"\n    input_data_missing_column = pd.DataFrame({\n        'price': [100.0, 200.0]\n    })\n\n    result = transform_data(input_data_missing_column)\n    expected_data = pd.DataFrame()\n\n    pd.testing.assert_frame_equal(result, expected_data)\n\n    input_data_missing_price = pd.DataFrame({\n        'payment_method': ['Credit Card', 'Cash']\n    })\n\n    result = transform_data(input_data_missing_price)\n\n    pd.testing.assert_frame_equal(result, expected_data)\n</code></pre> <ol> <li>Teste de exce\u00e7\u00e3o gen\u00e9rica (<code>test_transform_data_unexpected_exception</code>):<ul> <li>Objetivo: Verificar o tratamento de exce\u00e7\u00f5es inesperadas dentro da fun\u00e7\u00e3o <code>transform_data</code>, assegurando que um erro gen\u00e9rico resultar\u00e1 em um <code>DataFrame</code> vazio, sem interromper o fluxo da pipeline.</li> <li>Descri\u00e7\u00e3o: Um <code>DataFrame</code> de entrada com valores n\u00e3o num\u00e9ricos na coluna <code>price</code> \u00e9 passado para a fun\u00e7\u00e3o, induzindo um erro durante a convers\u00e3o de tipo. A fun\u00e7\u00e3o deve capturar essa exce\u00e7\u00e3o e retornar um <code>DataFrame</code> vazio.</li> <li>Valida\u00e7\u00e3o: Usa <code>pd.testing.assert_frame_equal</code> para garantir que o <code>DataFrame</code> retornado seja vazio, confirmando o tratamento robusto de erros.</li> <li>Fun\u00e7\u00e3o de teste: </li> </ul> </li> </ol> <p>Testa a fun\u00e7\u00e3o <code>transform_data</code> para verificar se uma exce\u00e7\u00e3o gen\u00e9rica \u00e9 tratada e um DataFrame vazio \u00e9 retornado.</p> Source code in <code>tests\\test_transform.py</code> <pre><code>def test_transform_data_unexpected_exception():\n    \"\"\"\n    Testa a fun\u00e7\u00e3o `transform_data` para verificar se uma exce\u00e7\u00e3o gen\u00e9rica \u00e9 tratada\n    e um DataFrame vazio \u00e9 retornado.\n    \"\"\"\n    input_data_invalid = pd.DataFrame({\n        'payment_method': ['Credit Card', 'Cash'],\n        'price': ['invalid_price', 'another_invalid']\n    })\n\n    result = transform_data(input_data_invalid)\n    expected_data = pd.DataFrame()\n\n    pd.testing.assert_frame_equal(result, expected_data)\n</code></pre>"},{"location":"transform/#consideracoes-finais","title":"Considera\u00e7\u00f5es finais","text":"<p>Esses testes cobrem os principais cen\u00e1rios que podem ocorrer ao processar dados reais com a fun\u00e7\u00e3o <code>transform_data</code>. A inclus\u00e3o de verifica\u00e7\u00f5es para colunas ausentes, tratamento de exce\u00e7\u00f5es gen\u00e9ricas e um caso positivo de sucesso proporcionam uma cobertura abrangente para garantir a estabilidade e a confiabilidade da fun\u00e7\u00e3o dentro da pipeline ETL. Esses testes permitem identificar rapidamente problemas em altera\u00e7\u00f5es futuras no c\u00f3digo, mantendo a qualidade do projeto.</p>"}]}