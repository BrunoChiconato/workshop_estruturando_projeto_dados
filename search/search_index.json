{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bem-vindo","text":"<p>Este projeto implementa uma pipeline de dados automatizada para extrair, transformar e carregar (ETL) dados de vendas fict\u00edcias de diversos formatos de arquivo para um banco de dados PostgreSQL na nuvem. A pipeline foi projetada para consolidar dados de m\u00faltiplas fontes em um \u00fanico DataFrame, aplicar transforma\u00e7\u00f5es espec\u00edficas e armazen\u00e1-los em um reposit\u00f3rio central para facilitar a an\u00e1lise e a consulta.</p>"},{"location":"#objetivo-do-projeto","title":"Objetivo do Projeto","text":"<p>A pipeline ETL realiza as seguintes opera\u00e7\u00f5es principais:</p> <ol> <li> <p>Extra\u00e7\u00e3o (Extract): Extrai dados de arquivos <code>.csv</code>, <code>.json</code> e <code>.parquet</code> localizados em um diret\u00f3rio espec\u00edfico. Cada arquivo cont\u00e9m informa\u00e7\u00f5es de vendas, como identificadores de pedidos, clientes, produtos, m\u00e9todo de pagamento, quantidade e pre\u00e7o.</p> </li> <li> <p>Transforma\u00e7\u00e3o (Transform): Processa os dados consolidados para calcular o valor total das vendas, agrupando-os por m\u00e9todo de pagamento. Esse processo resulta em um resumo que permite uma an\u00e1lise mais f\u00e1cil e r\u00e1pida dos m\u00e9todos de pagamento mais utilizados.</p> </li> <li> <p>Carga (Load): Insere os dados transformados em uma tabela chamada <code>sales_consolidated</code>, localizada em um banco de dados PostgreSQL hospedado na nuvem (Render). A tabela \u00e9 recriada a cada execu\u00e7\u00e3o, garantindo que os dados mais recentes estejam sempre dispon\u00edveis.</p> </li> </ol>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Python 3.12.5: Vers\u00e3o definida e gerenciada com <code>pyenv</code>.</li> <li>Poetry: Gerenciamento de depend\u00eancias e ambiente virtual.</li> <li>Pandas: Manipula\u00e7\u00e3o e processamento de dados.</li> <li>Pandera: Valida\u00e7\u00e3o de esquema para garantir a integridade dos dados.</li> <li>SQLAlchemy: Interface para conex\u00e3o com o banco de dados PostgreSQL.</li> <li>Loguru: Registro de logs de execu\u00e7\u00e3o e de erro para facilitar o monitoramento e a depura\u00e7\u00e3o.</li> <li>Render: Plataforma de hospedagem do banco de dados PostgreSQL, com uma camada gratuita para at\u00e9 certo volume de dados.</li> </ul>"},{"location":"#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Para executar este projeto, voc\u00ea precisar\u00e1 dos seguintes pr\u00e9-requisitos:</p> <ul> <li>Python 3.12.5: Instale e gerencie essa vers\u00e3o com o <code>pyenv</code>.</li> <li>Poetry: Gerenciador de depend\u00eancias e ambientes virtuais.</li> <li>Banco de Dados PostgreSQL: Um banco de dados PostgreSQL configurado na plataforma Render para armazenar os dados processados.</li> </ul>"},{"location":"#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<p>Aqui est\u00e1 uma vis\u00e3o geral da organiza\u00e7\u00e3o do projeto:</p> <pre><code>.\n\u251c\u2500\u2500 app/                 # C\u00f3digo principal da pipeline ETL\n\u251c\u2500\u2500 classes/             # Classes de suporte, incluindo a classe DataExtractor\n\u251c\u2500\u2500 data/                # Diret\u00f3rio para dados de entrada e sa\u00edda\n\u251c\u2500\u2500 decorators/          # Decoradores para logs e medi\u00e7\u00e3o de tempo de execu\u00e7\u00e3o\n\u251c\u2500\u2500 documentation/       # Documenta\u00e7\u00e3o do projeto (configurada para MkDocs)\n\u251c\u2500\u2500 funcs/               # Fun\u00e7\u00f5es auxiliares\n\u251c\u2500\u2500 tests/               # Testes automatizados do projeto\n\u251c\u2500\u2500 .env                 # Arquivo de vari\u00e1veis de ambiente (configura\u00e7\u00e3o do banco de dados)\n\u251c\u2500\u2500 .gitignore           # Arquivo para ignorar arquivos e diret\u00f3rios no Git\n\u251c\u2500\u2500 .python-version      # Vers\u00e3o Python especificada para pyenv (3.12.5)\n\u251c\u2500\u2500 app.log              # Arquivo de log gerado pela execu\u00e7\u00e3o do projeto\n\u251c\u2500\u2500 poetry.lock          # Arquivo de bloqueio de depend\u00eancias gerado pelo Poetry\n\u251c\u2500\u2500 pyproject.toml       # Arquivo de configura\u00e7\u00e3o do Poetry e depend\u00eancias\n\u2514\u2500\u2500 README.md            # Arquivo de documenta\u00e7\u00e3o inicial do projeto\n</code></pre>"},{"location":"#descricao-dos-diretorios-e-arquivos-principais","title":"Descri\u00e7\u00e3o dos Diret\u00f3rios e Arquivos Principais","text":"<ul> <li>app/: Cont\u00e9m o c\u00f3digo principal da pipeline ETL que executa as etapas de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga.</li> <li>classes/: Inclui classes auxiliares, como <code>DataExtractor</code>, para estruturar a extra\u00e7\u00e3o de dados de diferentes formatos.</li> <li>data/: Diret\u00f3rio para armazenamento de dados de entrada e processamento intermedi\u00e1rio.</li> <li>decorators/: Fun\u00e7\u00f5es decoradoras que adicionam funcionalidades de log e medi\u00e7\u00e3o de tempo \u00e0s fun\u00e7\u00f5es principais.</li> <li>documentation/: Diret\u00f3rio onde est\u00e3o armazenados os arquivos de documenta\u00e7\u00e3o para MkDocs, gerando a documenta\u00e7\u00e3o HTML do projeto.</li> <li>funcs/: Armazena fun\u00e7\u00f5es auxiliares, que s\u00e3o utilizadas para suportar o fluxo principal da pipeline.</li> <li>tests/: Cont\u00e9m testes automatizados para verificar a integridade das fun\u00e7\u00f5es e classes do projeto.</li> <li>.env: Arquivo de configura\u00e7\u00e3o para vari\u00e1veis de ambiente, onde s\u00e3o definidas as credenciais do banco de dados PostgreSQL.</li> <li>app.log: Arquivo de log gerado durante a execu\u00e7\u00e3o do projeto, \u00fatil para depura\u00e7\u00e3o e auditoria de processos.</li> <li>poetry.lock e pyproject.toml: Arquivos do Poetry para o gerenciamento de depend\u00eancias e configura\u00e7\u00e3o do ambiente virtual.</li> </ul>"},{"location":"#fluxo-da-pipeline-etl","title":"Fluxo da Pipeline ETL","text":"<p>Abaixo est\u00e1 uma vis\u00e3o geral do fluxo de dados dentro da pipeline ETL:</p> <pre><code>flowchart LR\n    A([.json]) --&gt; B[Extract]\n    C([.parquet]) --&gt; B\n    D([.csv]) --&gt; B\n    B --&gt;|Dados Consolidados| E[Transform]\n    E --&gt;|Dados Transformados| F[Load]\n    F --&gt; G[(Cloud PostgreSQL DB)]</code></pre>"},{"location":"#como-executar-o-projeto","title":"Como Executar o Projeto","text":"<ol> <li> <p>Clone este reposit\u00f3rio executando:</p> <pre><code>git clone git@github.com:BrunoChiconato/workshop_estruturando_projeto_dados.git\n</code></pre> </li> <li> <p>Verifique se o <code>pyenv</code> est\u00e1 instalado. A vers\u00e3o Python ser\u00e1 automaticamente configurada para 3.12.5 ao entrar no diret\u00f3rio do projeto, conforme especificado no arquivo <code>.python-version</code>.</p> </li> <li> <p>Instale o <code>Poetry</code> e todas as depend\u00eancias do projeto com o comando:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Gere arquivos de exemplo para a pipeline executando:</p> <pre><code>poetry run task gen_data\n</code></pre> </li> <li> <p>Execute o c\u00f3digo principal localizado em <code>pipeline.py</code> utilizando:</p> <pre><code>poetry run task main\n</code></pre> </li> <li> <p>Liste os outros comandos dispon\u00edveis neste projeto com:</p> <pre><code>poetry run task --list\n</code></pre> </li> </ol>"},{"location":"extract/","title":"Extra\u00e7\u00e3o dos Dados","text":""},{"location":"extract/#conteudo","title":"Conte\u00fado","text":"<p>Esta se\u00e7\u00e3o descreve o c\u00f3digo em Python localizado em <code>funcs/extract.py</code>, que extrai dados de diferentes tipos de arquivos, consolida-os em um DataFrame e retorna o DataFrame consolidado. Para alcan\u00e7ar esse objetivo, \u00e9 definida a fun\u00e7\u00e3o <code>extract_and_consolidate</code>.</p> <p>A fun\u00e7\u00e3o <code>extract_and_consolidate</code> utiliza a biblioteca <code>pandas</code> para a extra\u00e7\u00e3o e consolida\u00e7\u00e3o dos dados. Ela faz uso da classe <code>DataExtractor</code> (descrita em detalhes na se\u00e7\u00e3o Classes) e aplica dois decoradores: um para registrar logs e outro para calcular o tempo total de execu\u00e7\u00e3o da fun\u00e7\u00e3o, ambos explicados na se\u00e7\u00e3o Decoradores desta documenta\u00e7\u00e3o.</p> <p>A fun\u00e7\u00e3o recebe como par\u00e2metro o caminho do diret\u00f3rio onde os dados de entrada est\u00e3o armazenados. Em seguida, cria uma inst\u00e2ncia da classe <code>DataExtractor</code> e utiliza seus m\u00e9todos para ler dados em diferentes formatos, armazenando cada conjunto de dados em um DataFrame espec\u00edfico. Por fim, a fun\u00e7\u00e3o concatena todos esses DataFrames e retorna o resultado consolidado ao usu\u00e1rio.</p>"},{"location":"extract/#funcao-extract_and_consolidate","title":"Fun\u00e7\u00e3o <code>extract_and_consolidate</code>","text":"<p>Extrai e consolida dados de arquivos CSV, JSON e Parquet em um \u00fanico DataFrame.</p> <p>Esta fun\u00e7\u00e3o cria uma inst\u00e2ncia da classe DataExtractor para ler dados de diferentes formatos de arquivos (CSV, JSON e Parquet) presentes em um diret\u00f3rio espec\u00edfico. Ap\u00f3s a extra\u00e7\u00e3o, os dados s\u00e3o concatenados em um \u00fanico DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data_path</code> <code>str</code> <p>Caminho do diret\u00f3rio onde os arquivos CSV, JSON e Parquet est\u00e3o localizados.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame contendo os dados consolidados de todos os arquivos extra\u00eddos.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>Se algum dos arquivos CSV, JSON ou Parquet n\u00e3o for encontrado no diret\u00f3rio especificado.</p> Source code in <code>funcs\\extract.py</code> <pre><code>@time_decorador\n@log_decorator\ndef extract_and_consolidate(data_path: str) -&gt; pd.DataFrame:\n    \"\"\"\n    Extrai e consolida dados de arquivos CSV, JSON e Parquet em um \u00fanico DataFrame.\n\n    Esta fun\u00e7\u00e3o cria uma inst\u00e2ncia da classe DataExtractor para ler dados de diferentes formatos\n    de arquivos (CSV, JSON e Parquet) presentes em um diret\u00f3rio espec\u00edfico. Ap\u00f3s a extra\u00e7\u00e3o, os\n    dados s\u00e3o concatenados em um \u00fanico DataFrame.\n\n    Args:\n        data_path (str): Caminho do diret\u00f3rio onde os arquivos CSV, JSON e Parquet est\u00e3o localizados.\n\n    Returns:\n        pd.DataFrame: DataFrame contendo os dados consolidados de todos os arquivos extra\u00eddos.\n\n    Raises:\n        FileNotFoundError: Se algum dos arquivos CSV, JSON ou Parquet n\u00e3o for encontrado no diret\u00f3rio especificado.\n    \"\"\"\n    extractor = DataExtractor()\n\n    csv_data: pd.DataFrame = extractor.read_csv_data(input_path = data_path)\n    json_data: pd.DataFrame = extractor.read_json_data(input_path = data_path)\n    parquet_data: pd.DataFrame = extractor.read_parquet_data(input_path = data_path)\n\n    consolidate_data: pd.DataFrame = pd.concat([csv_data, json_data, parquet_data], ignore_index=True)\n    return consolidate_data\n</code></pre>"},{"location":"extract/#teste-unitario","title":"Teste unit\u00e1rio","text":"<p>Esse teste unit\u00e1rio verifica o comportamento correto da fun\u00e7\u00e3o <code>extract_and_consolidate</code> utilizando mocks para simular as opera\u00e7\u00f5es da classe <code>DataExtractor</code>. Isso permite validar a fun\u00e7\u00e3o sem realmente acessar o sistema de arquivos.</p>"},{"location":"extract/#aspectos-validados","title":"Aspectos validados","text":"<ol> <li>Valida\u00e7\u00e3o do caminho de entrada: O teste assegura que o m\u00e9todo <code>validate_input_path</code> aceita o caminho dos dados fornecido.</li> <li>Leitura de dados simulados:<ul> <li>S\u00e3o criados mocks para os m\u00e9todos <code>read_csv_data</code>, <code>read_json_data</code> e <code>read_parquet_data</code>, que retornam DataFrames pr\u00e9-definidos para simular dados CSV, JSON e Parquet.</li> <li>Esses mocks garantem que o teste \u00e9 isolado do sistema de arquivos real.</li> </ul> </li> <li>Consolida\u00e7\u00e3o dos dados:<ul> <li>O teste chama a fun\u00e7\u00e3o <code>extract_and_consolidate</code> e compara o DataFrame resultante com um DataFrame esperado que cont\u00e9m a consolida\u00e7\u00e3o dos dados simulados.</li> <li>A compara\u00e7\u00e3o \u00e9 feita com <code>pd.testing.assert_frame_equal</code>, que verifica se o DataFrame resultante \u00e9 id\u00eantico ao esperado em estrutura e valores.</li> </ul> </li> <li>Verifica\u00e7\u00e3o de chamadas:<ul> <li>O teste verifica que cada m\u00e9todo de leitura (<code>read_csv_data</code>, <code>read_json_data</code>, <code>read_parquet_data</code>) foi chamado exatamente uma vez com o caminho correto, usando <code>assert_called_once_with</code>.</li> </ul> </li> <li>Asser\u00e7\u00e3o final: Caso a sa\u00edda da fun\u00e7\u00e3o n\u00e3o corresponda ao DataFrame esperado, um erro de asser\u00e7\u00e3o \u00e9 levantado, indicando uma falha no comportamento esperado da fun\u00e7\u00e3o.</li> </ol>"},{"location":"extract/#funcao-test_extract_and_consolidate","title":"Fun\u00e7\u00e3o <code>test_extract_and_consolidate</code>","text":"<p>Testa a fun\u00e7\u00e3o <code>extract_and_consolidate</code> para verificar a consolida\u00e7\u00e3o de dados de m\u00faltiplas fontes (CSV, JSON e Parquet) em um \u00fanico DataFrame.</p> <p>Este teste simula os m\u00e9todos da classe <code>DataExtractor</code> usando mocks, evitando o acesso  ao sistema de arquivos e garantindo que a fun\u00e7\u00e3o <code>extract_and_consolidate</code> funcione corretamente. S\u00e3o validados os seguintes aspectos:</p> <ul> <li>O m\u00e9todo <code>validate_input_path</code> aceita o caminho dos dados fornecido.</li> <li>Os m\u00e9todos <code>read_csv_data</code>, <code>read_json_data</code> e <code>read_parquet_data</code> retornam DataFrames simulados    para os dados CSV, JSON e Parquet, respectivamente.</li> <li>A fun\u00e7\u00e3o <code>extract_and_consolidate</code> retorna um DataFrame consolidado com a estrutura e os valores esperados.</li> <li>Cada m\u00e9todo de leitura \u00e9 chamado exatamente uma vez com o caminho correto.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>MockDataExtractor</code> <code>MagicMock</code> <p>Mock da classe <code>DataExtractor</code> para simular a leitura de dados  e evitar acessos reais ao sistema de arquivos.</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>Se a sa\u00edda da fun\u00e7\u00e3o n\u00e3o corresponder ao DataFrame esperado.</p> Source code in <code>tests\\test_extract.py</code> <pre><code>@patch('funcs.extract.DataExtractor')\ndef test_extract_and_consolidate(MockDataExtractor):\n    \"\"\"\n    Testa a fun\u00e7\u00e3o `extract_and_consolidate` para verificar a consolida\u00e7\u00e3o de dados de m\u00faltiplas\n    fontes (CSV, JSON e Parquet) em um \u00fanico DataFrame.\n\n    Este teste simula os m\u00e9todos da classe `DataExtractor` usando mocks, evitando o acesso \n    ao sistema de arquivos e garantindo que a fun\u00e7\u00e3o `extract_and_consolidate` funcione corretamente.\n    S\u00e3o validados os seguintes aspectos:\n\n    - O m\u00e9todo `validate_input_path` aceita o caminho dos dados fornecido.\n    - Os m\u00e9todos `read_csv_data`, `read_json_data` e `read_parquet_data` retornam DataFrames simulados \n      para os dados CSV, JSON e Parquet, respectivamente.\n    - A fun\u00e7\u00e3o `extract_and_consolidate` retorna um DataFrame consolidado com a estrutura e os valores esperados.\n    - Cada m\u00e9todo de leitura \u00e9 chamado exatamente uma vez com o caminho correto.\n\n    Args:\n        MockDataExtractor (MagicMock): Mock da classe `DataExtractor` para simular a leitura de dados \n            e evitar acessos reais ao sistema de arquivos.\n\n    Raises:\n        AssertionError: Se a sa\u00edda da fun\u00e7\u00e3o n\u00e3o corresponder ao DataFrame esperado.\n    \"\"\"\n    mock_extractor = MockDataExtractor.return_value\n\n    mock_extractor.validate_input_path.side_effect = lambda path: path\n\n    mock_extractor.read_csv_data.return_value = pd.DataFrame({\n        'order_id': [1, 2], \n        'customer_id': [202, 448], \n        'order_date': ['2023-01-01', '2023-01-02'], \n        'product_id': [1484, 1027], \n        'quantity': [5, 4], \n        'price': [99.68, 20.8], \n        'payment_method': ['Cash', 'Debit Card'], \n        'store_location': ['Los Angeles', 'Houston']\n    })\n    mock_extractor.read_json_data.return_value = pd.DataFrame({\n        'order_id': [3], \n        'customer_id': [370], \n        'order_date': ['2023-01-03'], \n        'product_id': [1713], \n        'quantity': [6], \n        'price': [362.8], \n        'payment_method': ['Credit Card'], \n        'store_location': ['New York']\n    })\n    mock_extractor.read_parquet_data.return_value = pd.DataFrame({\n        'order_id': [4], \n        'customer_id': [206], \n        'order_date': ['2023-01-04'], \n        'product_id': [1038], \n        'quantity': [7], \n        'price': [214.82], \n        'payment_method': ['Credit Card'], \n        'store_location': ['Houston']\n    })\n\n    data_path = 'fake_path'\n\n    result = extract_and_consolidate(data_path)\n\n    mock_extractor.read_csv_data.assert_called_once_with(input_path=data_path)\n    mock_extractor.read_json_data.assert_called_once_with(input_path=data_path)\n    mock_extractor.read_parquet_data.assert_called_once_with(input_path=data_path)\n\n    expected_data = pd.DataFrame({\n        'order_id': [1, 2, 3, 4], \n        'customer_id': [202, 448, 370, 206], \n        'order_date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04'], \n        'product_id': [1484, 1027, 1713, 1038], \n        'quantity': [5, 4, 6, 7], \n        'price': [99.68, 20.8, 362.8, 214.82], \n        'payment_method': ['Cash', 'Debit Card', 'Credit Card', 'Credit Card'], \n        'store_location': ['Los Angeles', 'Houston', 'New York', 'Houston']\n    })\n\n    pd.testing.assert_frame_equal(result, expected_data)\n</code></pre>"},{"location":"generate_data/","title":"Gera\u00e7\u00e3o dos Dados","text":""},{"location":"generate_data/#conteudo","title":"Conte\u00fado","text":"<p>A presente se\u00e7\u00e3o inclui um c\u00f3digo em Python, localizado em <code>funcs/generate_data.py</code>, que gera dados fict\u00edcios de vendas para diversas finalidades. A fun\u00e7\u00e3o principal, <code>generate_data</code>, utiliza as bibliotecas <code>os</code>, <code>pandas</code> e <code>numpy</code> para criar dados simulados e salv\u00e1-los em formatos variados, incluindo <code>.csv</code>, <code>.json</code> e <code>.parquet</code>.</p> <p>Abaixo est\u00e1 uma descri\u00e7\u00e3o detalhada do funcionamento da fun\u00e7\u00e3o <code>generate_data</code>:</p> <ol> <li> <p>Par\u00e2metro de caminho: A fun\u00e7\u00e3o recebe uma string que especifica o caminho onde os dados gerados ser\u00e3o salvos. Esse caminho pode ser personalizado pelo usu\u00e1rio.</p> </li> <li> <p>Defini\u00e7\u00e3o do tamanho dos dados: O n\u00famero de linhas do conjunto de dados \u00e9 definido na fun\u00e7\u00e3o. Neste projeto, o valor padr\u00e3o \u00e9 <code>n = 1000</code>, mas pode ser ajustado conforme a necessidade.</p> </li> <li> <p>Cria\u00e7\u00e3o do DataFrame: Utilizando a biblioteca <code>pandas</code>, a fun\u00e7\u00e3o cria um DataFrame e define colunas que representam um relat\u00f3rio de vendas fict\u00edcio. Para popular essas colunas, s\u00e3o utilizados arrays gerados com a biblioteca <code>numpy</code>, simulando dados como identificadores de vendas, valores, datas e outros detalhes relevantes.</p> </li> <li> <p>Verifica\u00e7\u00e3o e cria\u00e7\u00e3o do caminho: A fun\u00e7\u00e3o verifica se o caminho fornecido existe. Caso o caminho n\u00e3o exista, ele \u00e9 criado automaticamente para garantir que os arquivos possam ser salvos.</p> </li> <li> <p>Salvamento dos arquivos: O DataFrame gerado \u00e9 salvo nos tr\u00eas formatos de arquivo especificados (<code>.csv</code>, <code>.json</code> e <code>.parquet</code>). Ap\u00f3s o salvamento, uma mensagem de sucesso \u00e9 exibida, confirmando que os arquivos foram gerados com sucesso.</p> </li> </ol>"},{"location":"generate_data/#funcao-generate_data","title":"Fun\u00e7\u00e3o <code>generate_data</code>","text":"<p>Gera dados fict\u00edcios de vendas e salva em CSV, Parquet e JSON.</p> <p>Esta fun\u00e7\u00e3o cria um conjunto de dados fict\u00edcios com as seguintes colunas:</p> <ul> <li>order_id (int)</li> <li>customer_id (int)</li> <li>order_date (date)</li> <li>product_id (int)</li> <li>quantity (int)</li> <li>price (float)</li> <li>payment_method (string)</li> <li>store_location (string)</li> </ul> <p>A fun\u00e7\u00e3o verifica se o diret\u00f3rio especificado pelo argumento 'path' existe. Caso o diret\u00f3rio n\u00e3o exista, ele ser\u00e1 criado. Em seguida, os dados s\u00e3o gerados e salvos nos formatos 'csv', 'parquet' e 'json'.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Caminho da pasta onde os arquivos ser\u00e3o salvos.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>funcs\\generate_data.py</code> <pre><code>def generate_data(path: str) -&gt; None:\n    \"\"\"\n    Gera dados fict\u00edcios de vendas e salva em CSV, Parquet e JSON.\n\n    Esta fun\u00e7\u00e3o cria um conjunto de dados fict\u00edcios com as seguintes colunas:\n\n    - order_id (int)\n    - customer_id (int)\n    - order_date (date)\n    - product_id (int)\n    - quantity (int)\n    - price (float)\n    - payment_method (string)\n    - store_location (string)\n\n    A fun\u00e7\u00e3o verifica se o diret\u00f3rio especificado pelo argumento 'path' existe.\n    Caso o diret\u00f3rio n\u00e3o exista, ele ser\u00e1 criado. Em seguida, os dados s\u00e3o gerados\n    e salvos nos formatos 'csv', 'parquet' e 'json'.\n\n    Args:\n        path (str): Caminho da pasta onde os arquivos ser\u00e3o salvos.\n\n    Returns:\n        None\n    \"\"\"\n    n: int = 1000\n    data: pd.DataFrame = pd.DataFrame({\n        'order_id': np.arange(1, n+1),\n        'customer_id': np.random.randint(100, 500, size=n),\n        'order_date': pd.date_range('2023-01-01', periods=n, freq='D'),\n        'product_id': np.random.randint(1000, 2000, size=n),\n        'quantity': np.random.randint(1, 10, size=n),\n        'price': np.round(np.random.uniform(10, 500, size=n), 2),\n        'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'Cash', 'PayPal'], size=n),\n        'store_location': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], size=n)\n    })\n\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)\n\n    data.to_csv(os.path.join(path, 'vendas_ficticias.csv'), index=False)\n    data.to_parquet(os.path.join(path,'vendas_ficticias.parquet'), index=False)\n    data.to_json(os.path.join(path,'vendas_ficticias.json'), orient='records', lines=True)\n\n    print('Arquivos criados com sucesso!')\n</code></pre>"}]}