{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bem-vindo","text":"<p>Este projeto implementa uma pipeline de dados automatizada para extrair, transformar e carregar (ETL) dados de vendas fict\u00edcias de diversos formatos de arquivo para um banco de dados PostgreSQL na nuvem. A pipeline foi projetada para consolidar dados de m\u00faltiplas fontes em um \u00fanico DataFrame, aplicar transforma\u00e7\u00f5es espec\u00edficas e armazen\u00e1-los em um reposit\u00f3rio central para facilitar a an\u00e1lise e a consulta.</p>"},{"location":"#objetivo-do-projeto","title":"Objetivo do Projeto","text":"<p>A pipeline ETL realiza as seguintes opera\u00e7\u00f5es principais:</p> <ol> <li> <p>Extra\u00e7\u00e3o (Extract): Extrai dados de arquivos <code>.csv</code>, <code>.json</code> e <code>.parquet</code> localizados em um diret\u00f3rio espec\u00edfico. Cada arquivo cont\u00e9m informa\u00e7\u00f5es de vendas, como identificadores de pedidos, clientes, produtos, m\u00e9todo de pagamento, quantidade e pre\u00e7o.</p> </li> <li> <p>Transforma\u00e7\u00e3o (Transform): Processa os dados consolidados para calcular o valor total das vendas, agrupando-os por m\u00e9todo de pagamento. Esse processo resulta em um resumo que permite uma an\u00e1lise mais f\u00e1cil e r\u00e1pida dos m\u00e9todos de pagamento mais utilizados.</p> </li> <li> <p>Carga (Load): Insere os dados transformados em uma tabela chamada <code>sales_consolidated</code>, localizada em um banco de dados PostgreSQL hospedado na nuvem (Render). A tabela \u00e9 recriada a cada execu\u00e7\u00e3o, garantindo que os dados mais recentes estejam sempre dispon\u00edveis.</p> </li> </ol>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<ul> <li>Python 3.12.5: Vers\u00e3o definida e gerenciada com <code>pyenv</code>.</li> <li>Poetry: Gerenciamento de depend\u00eancias e ambiente virtual.</li> <li>Pandas: Manipula\u00e7\u00e3o e processamento de dados.</li> <li>Pandera: Valida\u00e7\u00e3o de esquema para garantir a integridade dos dados.</li> <li>SQLAlchemy: Interface para conex\u00e3o com o banco de dados PostgreSQL.</li> <li>Loguru: Registro de logs de execu\u00e7\u00e3o e de erro para facilitar o monitoramento e a depura\u00e7\u00e3o.</li> <li>Render: Plataforma de hospedagem do banco de dados PostgreSQL, com uma camada gratuita para at\u00e9 certo volume de dados.</li> </ul>"},{"location":"#pre-requisitos","title":"Pr\u00e9-requisitos","text":"<p>Para executar este projeto, voc\u00ea precisar\u00e1 dos seguintes pr\u00e9-requisitos:</p> <ul> <li>Python 3.12.5: Instale e gerencie essa vers\u00e3o com o <code>pyenv</code>.</li> <li>Poetry: Gerenciador de depend\u00eancias e ambientes virtuais.</li> <li>Banco de Dados PostgreSQL: Um banco de dados PostgreSQL configurado na plataforma Render para armazenar os dados processados.</li> </ul>"},{"location":"#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<p>Aqui est\u00e1 uma vis\u00e3o geral da organiza\u00e7\u00e3o do projeto:</p> <pre><code>.\n\u251c\u2500\u2500 app/                 # C\u00f3digo principal da pipeline ETL\n\u251c\u2500\u2500 classes/             # Classes de suporte, incluindo a classe DataExtractor\n\u251c\u2500\u2500 data/                # Diret\u00f3rio para dados de entrada e sa\u00edda\n\u251c\u2500\u2500 decorators/          # Decoradores para logs e medi\u00e7\u00e3o de tempo de execu\u00e7\u00e3o\n\u251c\u2500\u2500 documentation/       # Documenta\u00e7\u00e3o do projeto (configurada para MkDocs)\n\u251c\u2500\u2500 funcs/               # Fun\u00e7\u00f5es auxiliares\n\u251c\u2500\u2500 tests/               # Testes automatizados do projeto\n\u251c\u2500\u2500 .env                 # Arquivo de vari\u00e1veis de ambiente (configura\u00e7\u00e3o do banco de dados)\n\u251c\u2500\u2500 .gitignore           # Arquivo para ignorar arquivos e diret\u00f3rios no Git\n\u251c\u2500\u2500 .python-version      # Vers\u00e3o Python especificada para pyenv (3.12.5)\n\u251c\u2500\u2500 app.log              # Arquivo de log gerado pela execu\u00e7\u00e3o do projeto\n\u251c\u2500\u2500 poetry.lock          # Arquivo de bloqueio de depend\u00eancias gerado pelo Poetry\n\u251c\u2500\u2500 pyproject.toml       # Arquivo de configura\u00e7\u00e3o do Poetry e depend\u00eancias\n\u2514\u2500\u2500 README.md            # Arquivo de documenta\u00e7\u00e3o inicial do projeto\n</code></pre>"},{"location":"#descricao-dos-diretorios-e-arquivos-principais","title":"Descri\u00e7\u00e3o dos Diret\u00f3rios e Arquivos Principais","text":"<ul> <li>app/: Cont\u00e9m o c\u00f3digo principal da pipeline ETL que executa as etapas de extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carga.</li> <li>classes/: Inclui classes auxiliares, como <code>DataExtractor</code>, para estruturar a extra\u00e7\u00e3o de dados de diferentes formatos.</li> <li>data/: Diret\u00f3rio para armazenamento de dados de entrada e processamento intermedi\u00e1rio.</li> <li>decorators/: Fun\u00e7\u00f5es decoradoras que adicionam funcionalidades de log e medi\u00e7\u00e3o de tempo \u00e0s fun\u00e7\u00f5es principais.</li> <li>documentation/: Diret\u00f3rio onde est\u00e3o armazenados os arquivos de documenta\u00e7\u00e3o para MkDocs, gerando a documenta\u00e7\u00e3o HTML do projeto.</li> <li>funcs/: Armazena fun\u00e7\u00f5es auxiliares, que s\u00e3o utilizadas para suportar o fluxo principal da pipeline.</li> <li>tests/: Cont\u00e9m testes automatizados para verificar a integridade das fun\u00e7\u00f5es e classes do projeto.</li> <li>.env: Arquivo de configura\u00e7\u00e3o para vari\u00e1veis de ambiente, onde s\u00e3o definidas as credenciais do banco de dados PostgreSQL.</li> <li>app.log: Arquivo de log gerado durante a execu\u00e7\u00e3o do projeto, \u00fatil para depura\u00e7\u00e3o e auditoria de processos.</li> <li>poetry.lock e pyproject.toml: Arquivos do Poetry para o gerenciamento de depend\u00eancias e configura\u00e7\u00e3o do ambiente virtual.</li> </ul>"},{"location":"#fluxo-da-pipeline-etl","title":"Fluxo da Pipeline ETL","text":"<p>Abaixo est\u00e1 uma vis\u00e3o geral do fluxo de dados dentro da pipeline ETL:</p> <pre><code>flowchart LR\n    A([.json]) --&gt; B[Extract]\n    C([.parquet]) --&gt; B\n    D([.csv]) --&gt; B\n    B --&gt;|Dados Consolidados| E[Transform]\n    E --&gt;|Dados Transformados| F[Load]\n    F --&gt; G[(Cloud PostgreSQL DB)]</code></pre>"},{"location":"#como-executar-o-projeto","title":"Como Executar o Projeto","text":"<ol> <li> <p>Clone este reposit\u00f3rio executando:</p> <pre><code>git clone git@github.com:BrunoChiconato/workshop_estruturando_projeto_dados.git\n</code></pre> </li> <li> <p>Verifique se o <code>pyenv</code> est\u00e1 instalado. A vers\u00e3o Python ser\u00e1 automaticamente configurada para 3.12.5 ao entrar no diret\u00f3rio do projeto, conforme especificado no arquivo <code>.python-version</code>.</p> </li> <li> <p>Instale o <code>Poetry</code> e todas as depend\u00eancias do projeto com o comando:</p> <pre><code>poetry install\n</code></pre> </li> <li> <p>Gere arquivos de exemplo para a pipeline executando:</p> <pre><code>poetry run task gen_data\n</code></pre> </li> <li> <p>Execute o c\u00f3digo principal localizado em <code>pipeline.py</code> utilizando:</p> <pre><code>poetry run task main\n</code></pre> </li> <li> <p>Liste os outros comandos dispon\u00edveis neste projeto com:</p> <pre><code>poetry run task --list\n</code></pre> </li> </ol>"},{"location":"generate_data/","title":"Gera\u00e7\u00e3o de Dados","text":""},{"location":"generate_data/#conteudo","title":"Conte\u00fado","text":"<p>A presente se\u00e7\u00e3o inclui um c\u00f3digo em Python, localizado em <code>funcs/generate_data.py</code>, que gera dados fict\u00edcios de vendas para diversas finalidades. A fun\u00e7\u00e3o principal, <code>generate_data</code>, utiliza as bibliotecas <code>os</code>, <code>pandas</code> e <code>numpy</code> para criar dados simulados e salv\u00e1-los em formatos variados, incluindo <code>.csv</code>, <code>.json</code> e <code>.parquet</code>.</p> <p>Abaixo est\u00e1 uma descri\u00e7\u00e3o detalhada do funcionamento da fun\u00e7\u00e3o <code>generate_data</code>:</p> <ol> <li> <p>Par\u00e2metro de caminho: A fun\u00e7\u00e3o recebe uma string que especifica o caminho onde os dados gerados ser\u00e3o salvos. Esse caminho pode ser personalizado pelo usu\u00e1rio.</p> </li> <li> <p>Defini\u00e7\u00e3o do tamanho dos dados: O n\u00famero de linhas do conjunto de dados \u00e9 definido na fun\u00e7\u00e3o. Neste projeto, o valor padr\u00e3o \u00e9 <code>n = 1000</code>, mas pode ser ajustado conforme a necessidade.</p> </li> <li> <p>Cria\u00e7\u00e3o do DataFrame: Utilizando a biblioteca <code>pandas</code>, a fun\u00e7\u00e3o cria um DataFrame e define colunas que representam um relat\u00f3rio de vendas fict\u00edcio. Para popular essas colunas, s\u00e3o utilizados arrays gerados com a biblioteca <code>numpy</code>, simulando dados como identificadores de vendas, valores, datas e outros detalhes relevantes.</p> </li> <li> <p>Verifica\u00e7\u00e3o e cria\u00e7\u00e3o do caminho: A fun\u00e7\u00e3o verifica se o caminho fornecido existe. Caso o caminho n\u00e3o exista, ele \u00e9 criado automaticamente para garantir que os arquivos possam ser salvos.</p> </li> <li> <p>Salvamento dos arquivos: O DataFrame gerado \u00e9 salvo nos tr\u00eas formatos de arquivo especificados (<code>.csv</code>, <code>.json</code> e <code>.parquet</code>). Ap\u00f3s o salvamento, uma mensagem de sucesso \u00e9 exibida, confirmando que os arquivos foram gerados com sucesso.</p> </li> </ol>"},{"location":"generate_data/#funcao-generate_data","title":"Fun\u00e7\u00e3o <code>generate_data</code>","text":"<p>Gera dados fict\u00edcios de vendas e salva em CSV, Parquet e JSON.</p> <p>Esta fun\u00e7\u00e3o cria um conjunto de dados fict\u00edcios com as seguintes colunas:</p> <ul> <li>order_id (int)</li> <li>customer_id (int)</li> <li>order_date (date)</li> <li>product_id (int)</li> <li>quantity (int)</li> <li>price (float)</li> <li>payment_method (string)</li> <li>store_location (string)</li> </ul> <p>A fun\u00e7\u00e3o verifica se o diret\u00f3rio especificado pelo argumento 'path' existe. Caso o diret\u00f3rio n\u00e3o exista, ele ser\u00e1 criado. Em seguida, os dados s\u00e3o gerados e salvos nos formatos 'csv', 'parquet' e 'json'.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Caminho da pasta onde os arquivos ser\u00e3o salvos.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>funcs\\generate_data.py</code> <pre><code>def generate_data(path: str) -&gt; None:\n    \"\"\"\n    Gera dados fict\u00edcios de vendas e salva em CSV, Parquet e JSON.\n\n    Esta fun\u00e7\u00e3o cria um conjunto de dados fict\u00edcios com as seguintes colunas:\n\n    - order_id (int)\n    - customer_id (int)\n    - order_date (date)\n    - product_id (int)\n    - quantity (int)\n    - price (float)\n    - payment_method (string)\n    - store_location (string)\n\n    A fun\u00e7\u00e3o verifica se o diret\u00f3rio especificado pelo argumento 'path' existe.\n    Caso o diret\u00f3rio n\u00e3o exista, ele ser\u00e1 criado. Em seguida, os dados s\u00e3o gerados\n    e salvos nos formatos 'csv', 'parquet' e 'json'.\n\n    Args:\n        path (str): Caminho da pasta onde os arquivos ser\u00e3o salvos.\n\n    Returns:\n        None\n    \"\"\"\n    n: int = 1000\n    data: pd.DataFrame = pd.DataFrame({\n        'order_id': np.arange(1, n+1),\n        'customer_id': np.random.randint(100, 500, size=n),\n        'order_date': pd.date_range('2023-01-01', periods=n, freq='D'),\n        'product_id': np.random.randint(1000, 2000, size=n),\n        'quantity': np.random.randint(1, 10, size=n),\n        'price': np.round(np.random.uniform(10, 500, size=n), 2),\n        'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'Cash', 'PayPal'], size=n),\n        'store_location': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], size=n)\n    })\n\n    if not os.path.exists(path):\n        os.makedirs(path, exist_ok=True)\n\n    data.to_csv(os.path.join(path, 'vendas_ficticias.csv'), index=False)\n    data.to_parquet(os.path.join(path,'vendas_ficticias.parquet'), index=False)\n    data.to_json(os.path.join(path,'vendas_ficticias.json'), orient='records', lines=True)\n\n    print('Arquivos criados com sucesso!')\n</code></pre>"}]}